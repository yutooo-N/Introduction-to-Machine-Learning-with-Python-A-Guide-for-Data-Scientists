{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"picture_large978-4-87311-798-0.jpeg\" width=\"300px\">\n",
    "<p><pre>目次\n",
    "まえがき\n",
    "\n",
    "1章　はじめに\n",
    "    1.1　なぜ機械学習なのか？\n",
    "        1.1.1　機械学習問題へのアプローチ械学習で解決可能な問題\n",
    "        1.1.2　タスクを知り、データを知る\n",
    "    1.2　なぜ Pythonなのか？\n",
    "    1.3　scikit-learn\n",
    "        1.3.1　scikit-learnのインストール\n",
    "    1.4　必要なライブラリとツール\n",
    "        1.4.1　Jupyter Notebook\n",
    "        1.4.2　NumPy\n",
    "        1.4.3　SciPy\n",
    "        1.4.4　matplotlib\n",
    "        1.4.5　pandas\n",
    "        1.4.6　mglearn\n",
    "    1.5　Python 2 vs. Python 3\n",
    "    1.6　本書で用いているバージョン\n",
    "    1.7　最初のアプリケーション：アイリスのクラス分類\n",
    "        1.7.1　データを読む\n",
    "        1.7.2　成功度合いの測定：訓練データとテストデータ\n",
    "        1.7.3　最初にすべきこと：データをよく観察する\n",
    "        1.7.4　最初のモデル： k-最近傍法\n",
    "        1.7.5　予測を行う\n",
    "        1.7.6　モデルの評価\n",
    "    1.8　まとめと今後の展望\n",
    "\n",
    "2章　教師あり学習\n",
    "    2.1　クラス分類と回帰\n",
    "    2.2　汎化、過剰適合、適合不足\n",
    "        2.2.1　モデルの複雑さとデータセットの大きさ\n",
    "    2.3　教師あり機械学習アルゴリズム\n",
    "        2.3.1　サンプルデータセット\n",
    "        2.3.2　k-最近傍法\n",
    "        2.3.3　線形モデル\n",
    "        2.3.4　ナイーブベイズクラス分類器\n",
    "        2.3.5　決定木\n",
    "        2.3.6　決定木のアンサンブル法\n",
    "        2.3.7　カーネル法を用いたサポートベクタマシン\n",
    "        2.3.8　ニューラルネットワーク（ディープラーニング）\n",
    "    2.4　クラス分類器の不確実性推定\n",
    "        2.4.1　決定関数（ Decision Function）\n",
    "        2.4.2　確率の予測\n",
    "        2.4.3　多クラス分類の不確実性\n",
    "    2.5　まとめと展望\n",
    "\n",
    "3章　教師なし学習と前処理\n",
    "    3.1　教師なし学習の種類\n",
    "    3.2　教師なし学習の難しさ\n",
    "    3.3　前処理とスケール変換\n",
    "        3.3.1　さまざまな前処理\n",
    "        3.3.2　データ変換の適用\n",
    "        3.3.3　訓練データとテストデータを同じように変換する\n",
    "        3.3.4　教師あり学習における前処理の効果\n",
    "    3.4　次元削減、特徴量抽出、多様体学習\n",
    "        3.4.1　主成分分析（ PCA）\n",
    "        3.4.2　非負値行列因子分解（ NMF）\n",
    "        3.4.3　t-SNEを用いた多様体学習\n",
    "    3.5　クラスタリング\n",
    "        3.5.1　k-meansクラスタリング\n",
    "        3.5.2　凝集型クラスタリング\n",
    "        3.5.3　DBSCAN\n",
    "        3.5.4　クラスタリングアルゴリズムの比較と評価\n",
    "        3.5.5　クラスタリング手法のまとめ\n",
    "    3.6　まとめと展望\n",
    "\n",
    "4章　データの表現と特徴量エンジニアリング\n",
    "    4.1　カテゴリ変数\n",
    "        4.1.1　ワンホットエンコーディング（ダミー変数）\n",
    "        4.1.2　数値でエンコードされているカテゴリ\n",
    "    4.2　ビニング、離散化、線形モデル、決定木\n",
    "    4.3　交互作用と多項式\n",
    "    4.4　単変量非線形変換\n",
    "    4.5　自動特徴量選択\n",
    "        4.5.1　単変量統計\n",
    "        4.5.2　モデルベース特徴量選択\n",
    "        4.5.3　反復特徴量選択\n",
    "    4.6　専門家知識の利用\n",
    "    4.7　まとめと展望\n",
    "\n",
    "5章　モデルの評価と改良\n",
    "    5.1　交差検証\n",
    "        5.1.1　scikit-learnでの交差検証\n",
    "        5.1.2　交差検証の利点\n",
    "        5.1.3　層化 k分割交差検証と他の戦略\n",
    "    5.2　グリッドサーチ\n",
    "        5.2.1　単純なグリッドサーチ\n",
    "        5.2.2　パラメータの過剰適合の危険性と検証セット\n",
    "        5.2.3　交差検証を用いたグリッドサーチ\n",
    "    5.3　評価基準とスコア\n",
    "        5.3.1　最終的な目標を見失わないこと\n",
    "        5.3.2　2クラス分類における基準\n",
    "        5.3.3　多クラス分類の基準\n",
    "        5.3.4　回帰の基準\n",
    "        5.3.5　評価基準を用いたモデル選択\n",
    "    5.4　まとめと展望\n",
    "\n",
    "6章　アルゴリズムチェーンとパイプライン\n",
    "    6.1　前処理を行う際のパラメータ選択\n",
    "    6.2　パイプラインの構築\n",
    "    6.3　パイプラインを用いたグリッドサーチ\n",
    "    6.4　汎用パイプラインインターフェイス\n",
    "        6.4.1　make_pipelineによる簡便なパイプライン生成\n",
    "        6.4.2　ステップ属性へのアクセス\n",
    "        6.4.3　GridSearchCV内のパイプラインの属性へのアクセス\n",
    "    6.5　前処理ステップとモデルパラメータに対するグリッドサーチ\n",
    "    6.6　グリッドサーチによるモデルの選択\n",
    "    6.7　まとめと展望\n",
    "\n",
    "7章　テキストデータの処理\n",
    "    7.1　文字列として表現されているデータのタイプ\n",
    "    7.2　例題アプリケーション：映画レビューのセンチメント分析\n",
    "    7.3　Bag of Wordsによるテキスト表現\n",
    "        7.3.1　トイデータセットに対する BoW\n",
    "        7.3.2　映画レビューの BoW\n",
    "    7.4　ストップワード\n",
    "    7.5　tf.idfを用いたデータのスケール変換\n",
    "    7.6　モデル係数の調査\n",
    "    7.7　1単語よりも大きい単位の Bag-of-Words (n-グラム )\n",
    "    7.8　より進んだトークン分割、語幹処理、見出し語化\n",
    "    7.9　トピックモデリングと文書クラスタリング\n",
    "        7.9.1　LDA（Latent Dirichlet Allocation）\n",
    "    7.10　まとめと展望\n",
    "\n",
    "8章　おわりに\n",
    "    8.1　機械学習問題へのアプローチ\n",
    "        8.1.1　人間をループに組み込む\n",
    "    8.2　プロトタイプから運用システムへ\n",
    "    8.3　運用システムのテスト\n",
    "    8.4　独自 Estimatorの構築\n",
    "    8.5　ここからどこへ行くのか\n",
    "        8.5.1　理論\n",
    "        8.5.2　他の機械学習フレームワークとパッケージ\n",
    "        8.5.3　ランキング、推薦システム、その他の学習\n",
    "        8.5.4　確率モデル、推論、確率プログラミング\n",
    "        8.5.5　ニューラルネットワーク\n",
    "        8.5.6　大規模データセットへのスケール\n",
    "        8.5.7　名誉を得る\n",
    "    8.6　結論\n",
    "\n",
    "索引\n",
    "</pre></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
