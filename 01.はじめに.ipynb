{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "\n",
    "\n",
    "\n",
    "真の前置き的なやつ。\n",
    "\n",
    "## なぜ機械学習なのか\n",
    "\n",
    "- if〜then〜のエキスパートシステムには限界がある。\n",
    "    - ロジックがドメイン固有のものになり、タスクが変化したら作り直さないといけない。\n",
    "    - 人間のエキスパートの思考に対する深い理解が必要。\n",
    "    - 顔認識とかはちょっと無理。\n",
    "    \n",
    "### 機械学習で解決可能な問題\n",
    "\n",
    "世の中には教師のある学習と教師のない学習がある。\n",
    "\n",
    "- **教師あり学習**\n",
    "    - 入力と出力のペアを与えてアルゴリズムを学習させる。\n",
    "    - 未知の入力に対する出力を予測する。\n",
    "    - データセットを集めることが大変な場合もある。\n",
    "- **教師なし学習**\n",
    "    - 出力のないデータセットでアルゴリズムを学習させる。\n",
    "    - トピック解析やクラスタリング、異常検知など。\n",
    "\n",
    "サンプルと特徴量。\n",
    "\n",
    "- **サンプル**\n",
    "     - 個々のエンティティまたは量。\n",
    "- **特徴量**\n",
    "    - エンティティの持つ特性を表現する列。\n",
    "    \n",
    "*情報量のないデータからは学習できない*ことを覚えておくこと。\n",
    "\n",
    "### タスクを知り、データを知る\n",
    "\n",
    "- よくデータを理解すること\n",
    "    - そのデータでそもそも問題を解決できるのか？\n",
    "    - どんな機械学習の問題に置き換えるべきか？\n",
    "    - データの数は十分か？\n",
    "\n",
    "アルゴリズムは問題の一部にすぎない。全体を心に留めておくこと。\n",
    "\n",
    "## なぜPythonなのか？\n",
    "\n",
    "- つよい\n",
    "  - 汎用言語の強さとドメイン特価スクリプト言語の強さがある。\n",
    "- Jupyter Notebookが使える\n",
    "  - データ解析はインタラクティブな過程。\n",
    "  \n",
    "## scikit-learn\n",
    "\n",
    "- Pythonで機械学習といったらこれ\n",
    "- オープンソース\n",
    "- ユーザーガイドも読んでおこうな\n",
    "    - http://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "### インストール\n",
    "\n",
    "```bash\n",
    "$ pip install numpy scipy matplotlib ipython scikit-learn pandas pillow\n",
    "```\n",
    "\n",
    "この他、章によってはgraphvizが必要な部分もある。\n",
    "\n",
    "## 必要なライブラリとツール\n",
    "\n",
    "- Jupyter Notebook\n",
    "    - ブラウザ上でMarkdownとコード混ぜて書けるやつ。\n",
    "- NumPy\n",
    "    - 配列計算ﾒｯﾁｬﾊﾔｲやつ\n",
    "- SciPy\n",
    "    - 科学技術計算できるやつ\n",
    "- matplotlib\n",
    "    - プロットするやつ\n",
    "- pandas\n",
    "    - Rのデータフレーム的な感じでデータ扱えるやつ\n",
    "- mglearn\n",
    "    - 「こちらに予め調理したものがあります」的に教科書の例試せるやつ\n",
    "  \n",
    "## Python 2 vs. Python 3\n",
    "\n",
    "3でやれ。\n",
    "\n",
    "## 最初のアプリケーション: アイリスのクラス分類\n",
    "\n",
    "- **みんな大好きiris**。\n",
    "- 以下の記事が詳しい。\n",
    "    - [irisの正体 (R Advent Calendar 2012 6日目) - どんな鳥も](http://d.hatena.ne.jp/tsutatsutatsuta/20121206/1354737461)\n",
    "\n",
    "### データを読む\n",
    "\n",
    "- `scikit-learn`の`datasets`モジュール\n",
    "    - いろんなサンプルデータセットが入ってる。\n",
    "    - `sklearn.datasets.load_iris()`で**iris**が返ってくるので適当に受けよう。\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "```\n",
    "\n",
    "`load_iris()`は**Bunch**クラスのオブジェクトを返す。これはディクショナリみたいに扱える。値には`iris_dataset['data']`以外に`iris_dataset.data`みたいにアクセスしてもいい。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.keys())\n",
    " ## dict_keys(['target_names', 'data', 'target', 'DESCR', 'feature_names'])\n",
    "```\n",
    "\n",
    "キー**DESCR**の中にデータセットの説明が入っている。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.DESCR)\n",
    " ## Iris Plants Database\n",
    " ## ====================\n",
    " ## \n",
    " ## Notes\n",
    " ## -----\n",
    " ## Data Set Characteristics:\n",
    " ##     :Number of Instances: 150 (50 in each of three classes)\n",
    " ##     :Number of Attributes: 4 numeric, predictive attributes and the class\n",
    " ##     :Attribute Information:\n",
    " ##         - sepal length in cm\n",
    " ##         - sepal width in cm\n",
    " ##         - petal length in cm\n",
    " ##         - petal width in cm\n",
    " ##         - class:\n",
    " ##                 - Iris-Setosa\n",
    " ##                 - Iris-Versicolour\n",
    " ##                 - Iris-Virginica\n",
    " ##     :Summary Statistics:\n",
    " ## \n",
    " ##     ============== ==== ==== ======= ===== ====================\n",
    " ##                     Min  Max   Mean    SD   Class Correlation\n",
    " ##     ============== ==== ==== ======= ===== ====================\n",
    " ##     sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
    " ##     sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
    " ##     petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
    " ##     petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
    " ##     ============== ==== ==== ======= ===== ====================\n",
    " ## \n",
    " ##     :Missing Attribute Values: None\n",
    " ##     :Class Distribution: 33.3% for each of 3 classes.\n",
    " ##     :Creator: R.A. Fisher\n",
    " ##     :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    " ##     :Date: July, 1988\n",
    " ## \n",
    " ## This is a copy of UCI ML iris datasets.\n",
    " ## http://archive.ics.uci.edu/ml/datasets/Iris\n",
    " ## \n",
    " ## The famous Iris database, first used by Sir R.A Fisher\n",
    " ## \n",
    " ## This is perhaps the best known database to be found in the\n",
    " ## pattern recognition literature.  Fisher's paper is a classic in the field and\n",
    " ## is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
    " ## data set contains 3 classes of 50 instances each, where each class refers to a\n",
    " ## type of iris plant.  One class is linearly separable from the other 2; the\n",
    " ## latter are NOT linearly separable from each other.\n",
    " ## \n",
    " ## References\n",
    " ## ----------\n",
    " ##    - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
    " ##      Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
    " ##      Mathematical Statistics\" (John Wiley, NY, 1950).\n",
    " ##    - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
    " ##      (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
    " ##    - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
    " ##      Structure and Classification Rule for Recognition in Partially Exposed\n",
    " ##      Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
    " ##      Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
    " ##    - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
    " ##      on Information Theory, May 1972, 431-433.\n",
    " ##    - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
    " ##      conceptual clustering system finds 3 classes in the data.\n",
    " ##    - Many, many more ...\n",
    "```\n",
    "\n",
    "**target**は符号化されていて、対応する名前は**target_names**に入っている。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.target)\n",
    " ## [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    " ##  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
    " ##  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
    " ##  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
    " ##  2 2]\n",
    "print(iris_dataset.target_names)\n",
    " ## ['setosa' 'versicolor' 'virginica']\n",
    "```\n",
    "\n",
    "要するに**target**で**target_names**を参照すると実際の目的変数の構造が見える。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.target_names[iris_dataset.target])\n",
    " ## ['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
    " ##  'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
    " ##  'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
    " ##  'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
    " ##  'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
    " ##  'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
    " ##  'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
    " ##  'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
    " ##  'virginica' 'virginica' 'virginica']\n",
    "```\n",
    "\n",
    "特徴量は**データ件数**×**特徴量数**のNumPy配列として**data**に入っている。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.data)\n",
    " ## [[5.1 3.5 1.4 0.2]\n",
    " ##  [4.9 3.  1.4 0.2]\n",
    " ##  [4.7 3.2 1.3 0.2]\n",
    " ##  [4.6 3.1 1.5 0.2]\n",
    " ##  [5.  3.6 1.4 0.2]\n",
    " ##  [5.4 3.9 1.7 0.4]\n",
    " ##  [4.6 3.4 1.4 0.3]\n",
    " ##  [5.  3.4 1.5 0.2]\n",
    " ##  [4.4 2.9 1.4 0.2]\n",
    " ##  [4.9 3.1 1.5 0.1]\n",
    " ##  [5.4 3.7 1.5 0.2]\n",
    " ##  [4.8 3.4 1.6 0.2]\n",
    " ##  [4.8 3.  1.4 0.1]\n",
    " ##  [4.3 3.  1.1 0.1]\n",
    " ##  [5.8 4.  1.2 0.2]\n",
    " ##  [5.7 4.4 1.5 0.4]\n",
    " ##  [5.4 3.9 1.3 0.4]\n",
    " ##  [5.1 3.5 1.4 0.3]\n",
    " ##  [5.7 3.8 1.7 0.3]\n",
    " ##  [5.1 3.8 1.5 0.3]\n",
    " ##  [5.4 3.4 1.7 0.2]\n",
    " ##  [5.1 3.7 1.5 0.4]\n",
    " ##  [4.6 3.6 1.  0.2]\n",
    " ##  [5.1 3.3 1.7 0.5]\n",
    " ##  [4.8 3.4 1.9 0.2]\n",
    " ##  [5.  3.  1.6 0.2]\n",
    " ##  [5.  3.4 1.6 0.4]\n",
    " ##  [5.2 3.5 1.5 0.2]\n",
    " ##  [5.2 3.4 1.4 0.2]\n",
    " ##  [4.7 3.2 1.6 0.2]\n",
    " ##  [4.8 3.1 1.6 0.2]\n",
    " ##  [5.4 3.4 1.5 0.4]\n",
    " ##  [5.2 4.1 1.5 0.1]\n",
    " ##  [5.5 4.2 1.4 0.2]\n",
    " ##  [4.9 3.1 1.5 0.1]\n",
    " ##  [5.  3.2 1.2 0.2]\n",
    " ##  [5.5 3.5 1.3 0.2]\n",
    " ##  [4.9 3.1 1.5 0.1]\n",
    " ##  [4.4 3.  1.3 0.2]\n",
    " ##  [5.1 3.4 1.5 0.2]\n",
    " ##  [5.  3.5 1.3 0.3]\n",
    " ##  [4.5 2.3 1.3 0.3]\n",
    " ##  [4.4 3.2 1.3 0.2]\n",
    " ##  [5.  3.5 1.6 0.6]\n",
    " ##  [5.1 3.8 1.9 0.4]\n",
    " ##  [4.8 3.  1.4 0.3]\n",
    " ##  [5.1 3.8 1.6 0.2]\n",
    " ##  [4.6 3.2 1.4 0.2]\n",
    " ##  [5.3 3.7 1.5 0.2]\n",
    " ##  [5.  3.3 1.4 0.2]\n",
    " ##  [7.  3.2 4.7 1.4]\n",
    " ##  [6.4 3.2 4.5 1.5]\n",
    " ##  [6.9 3.1 4.9 1.5]\n",
    " ##  [5.5 2.3 4.  1.3]\n",
    " ##  [6.5 2.8 4.6 1.5]\n",
    " ##  [5.7 2.8 4.5 1.3]\n",
    " ##  [6.3 3.3 4.7 1.6]\n",
    " ##  [4.9 2.4 3.3 1. ]\n",
    " ##  [6.6 2.9 4.6 1.3]\n",
    " ##  [5.2 2.7 3.9 1.4]\n",
    " ##  [5.  2.  3.5 1. ]\n",
    " ##  [5.9 3.  4.2 1.5]\n",
    " ##  [6.  2.2 4.  1. ]\n",
    " ##  [6.1 2.9 4.7 1.4]\n",
    " ##  [5.6 2.9 3.6 1.3]\n",
    " ##  [6.7 3.1 4.4 1.4]\n",
    " ##  [5.6 3.  4.5 1.5]\n",
    " ##  [5.8 2.7 4.1 1. ]\n",
    " ##  [6.2 2.2 4.5 1.5]\n",
    " ##  [5.6 2.5 3.9 1.1]\n",
    " ##  [5.9 3.2 4.8 1.8]\n",
    " ##  [6.1 2.8 4.  1.3]\n",
    " ##  [6.3 2.5 4.9 1.5]\n",
    " ##  [6.1 2.8 4.7 1.2]\n",
    " ##  [6.4 2.9 4.3 1.3]\n",
    " ##  [6.6 3.  4.4 1.4]\n",
    " ##  [6.8 2.8 4.8 1.4]\n",
    " ##  [6.7 3.  5.  1.7]\n",
    " ##  [6.  2.9 4.5 1.5]\n",
    " ##  [5.7 2.6 3.5 1. ]\n",
    " ##  [5.5 2.4 3.8 1.1]\n",
    " ##  [5.5 2.4 3.7 1. ]\n",
    " ##  [5.8 2.7 3.9 1.2]\n",
    " ##  [6.  2.7 5.1 1.6]\n",
    " ##  [5.4 3.  4.5 1.5]\n",
    " ##  [6.  3.4 4.5 1.6]\n",
    " ##  [6.7 3.1 4.7 1.5]\n",
    " ##  [6.3 2.3 4.4 1.3]\n",
    " ##  [5.6 3.  4.1 1.3]\n",
    " ##  [5.5 2.5 4.  1.3]\n",
    " ##  [5.5 2.6 4.4 1.2]\n",
    " ##  [6.1 3.  4.6 1.4]\n",
    " ##  [5.8 2.6 4.  1.2]\n",
    " ##  [5.  2.3 3.3 1. ]\n",
    " ##  [5.6 2.7 4.2 1.3]\n",
    " ##  [5.7 3.  4.2 1.2]\n",
    " ##  [5.7 2.9 4.2 1.3]\n",
    " ##  [6.2 2.9 4.3 1.3]\n",
    " ##  [5.1 2.5 3.  1.1]\n",
    " ##  [5.7 2.8 4.1 1.3]\n",
    " ##  [6.3 3.3 6.  2.5]\n",
    " ##  [5.8 2.7 5.1 1.9]\n",
    " ##  [7.1 3.  5.9 2.1]\n",
    " ##  [6.3 2.9 5.6 1.8]\n",
    " ##  [6.5 3.  5.8 2.2]\n",
    " ##  [7.6 3.  6.6 2.1]\n",
    " ##  [4.9 2.5 4.5 1.7]\n",
    " ##  [7.3 2.9 6.3 1.8]\n",
    " ##  [6.7 2.5 5.8 1.8]\n",
    " ##  [7.2 3.6 6.1 2.5]\n",
    " ##  [6.5 3.2 5.1 2. ]\n",
    " ##  [6.4 2.7 5.3 1.9]\n",
    " ##  [6.8 3.  5.5 2.1]\n",
    " ##  [5.7 2.5 5.  2. ]\n",
    " ##  [5.8 2.8 5.1 2.4]\n",
    " ##  [6.4 3.2 5.3 2.3]\n",
    " ##  [6.5 3.  5.5 1.8]\n",
    " ##  [7.7 3.8 6.7 2.2]\n",
    " ##  [7.7 2.6 6.9 2.3]\n",
    " ##  [6.  2.2 5.  1.5]\n",
    " ##  [6.9 3.2 5.7 2.3]\n",
    " ##  [5.6 2.8 4.9 2. ]\n",
    " ##  [7.7 2.8 6.7 2. ]\n",
    " ##  [6.3 2.7 4.9 1.8]\n",
    " ##  [6.7 3.3 5.7 2.1]\n",
    " ##  [7.2 3.2 6.  1.8]\n",
    " ##  [6.2 2.8 4.8 1.8]\n",
    " ##  [6.1 3.  4.9 1.8]\n",
    " ##  [6.4 2.8 5.6 2.1]\n",
    " ##  [7.2 3.  5.8 1.6]\n",
    " ##  [7.4 2.8 6.1 1.9]\n",
    " ##  [7.9 3.8 6.4 2. ]\n",
    " ##  [6.4 2.8 5.6 2.2]\n",
    " ##  [6.3 2.8 5.1 1.5]\n",
    " ##  [6.1 2.6 5.6 1.4]\n",
    " ##  [7.7 3.  6.1 2.3]\n",
    " ##  [6.3 3.4 5.6 2.4]\n",
    " ##  [6.4 3.1 5.5 1.8]\n",
    " ##  [6.  3.  4.8 1.8]\n",
    " ##  [6.9 3.1 5.4 2.1]\n",
    " ##  [6.7 3.1 5.6 2.4]\n",
    " ##  [6.9 3.1 5.1 2.3]\n",
    " ##  [5.8 2.7 5.1 1.9]\n",
    " ##  [6.8 3.2 5.9 2.3]\n",
    " ##  [6.7 3.3 5.7 2.5]\n",
    " ##  [6.7 3.  5.2 2.3]\n",
    " ##  [6.3 2.5 5.  1.9]\n",
    " ##  [6.5 3.  5.2 2. ]\n",
    " ##  [6.2 3.4 5.4 2.3]\n",
    " ##  [5.9 3.  5.1 1.8]]\n",
    "```\n",
    "\n",
    "**data**の各列は1種類の特徴量に対応するが、具体的になんという特徴量なのかは**feature_names**に入っている。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.feature_names)\n",
    " ## ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "```\n",
    "\n",
    "NumPy配列の扱い方を簡単に。配列の形状(次元)はshapeで取得できる。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.data.shape)\n",
    " ## (150, 4)\n",
    "```\n",
    "\n",
    "普通の配列みたいにスライスできる。\n",
    "\n",
    "\n",
    "```python\n",
    "print(iris_dataset.data[:5])\n",
    " ## [[5.1 3.5 1.4 0.2]\n",
    " ##  [4.9 3.  1.4 0.2]\n",
    " ##  [4.7 3.2 1.3 0.2]\n",
    " ##  [4.6 3.1 1.5 0.2]\n",
    " ##  [5.  3.6 1.4 0.2]]\n",
    "```\n",
    "\n",
    "### 成功度合いの測定: 訓練データとテストデータ\n",
    "\n",
    "- **未知の測定値からアヤメの品種を予測する**みたいなことやりたい。\n",
    "- **未知の入力**に対する予測能力 is **汎化能力**。\n",
    "    - こいつを最大化するのが目的。\n",
    "- 訓練に使ったデータはモデルの評価に使えない。\n",
    "    - そのデータに対してモデルを最適化しているので、良い性能が出るのは当然。\n",
    "    - 一部だけ訓練に使って残りをテスト用に取っておけばよいのでは？ → **ホールドアウト法**\n",
    "    - 訓練に使うデータセット: **訓練データ** or **訓練セット**。\n",
    "    - テストに使うデータセット: **テストデータ** or **テストセット**。\n",
    "    - **scikit-learn**でホールドアウト法をやるなら**model_selection.train_test_split**。\n",
    "- **scikit-learn**的な慣習\n",
    "    - (入力)データは**X**で表す\n",
    "    - データラベル(=入力に対応する出力)は**y**で表す\n",
    "    - 入力は行列で出力はベクトルなので、$f(\\textbf{X}) = \\textbf{y}$ということらしい。\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  iris_dataset.data, iris_dataset.target, random_state = 0\n",
    ")\n",
    "```\n",
    "\n",
    "- 引数**random_state**は乱数種を固定する。再現性確保のため。\n",
    "- デフォルトで訓練:テスト=3:1に分解する。\n",
    "\n",
    "\n",
    "```python\n",
    "print(X_train.shape)\n",
    " ## (112, 4)\n",
    "print(y_train.shape)\n",
    " ## (112,)\n",
    "print(X_test.shape)\n",
    " ## (38, 4)\n",
    "print(y_test.shape)\n",
    " ## (38,)\n",
    "```\n",
    "\n",
    "### 最初にすべきこと: データを良く観察する\n",
    "\n",
    "- まずは散布図を作れ\n",
    "- 多変量ならペアプロット(散布図行列)を作れ\n",
    "    - **pandas**にペアプロット作成関数があるので、**pandas.DataFrame**に変換して作業するとよい。\n",
    "    \n",
    "\n",
    "```python\n",
    "## DataFrameへの変換\n",
    "import pandas as pd\n",
    "iris_dataframe = pd.DataFrame(X_train, columns = iris_dataset.feature_names)\n",
    "```\n",
    "\n",
    "- 注: テキストで指定しているオプションの大半は外観調整のためのものなので、なくてもいい。\n",
    "\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "## プロット\n",
    "# pandas.scatter_matrixはdeprecated\n",
    "pd.plotting.scatter_matrix( \n",
    "  iris_dataframe,    # データの指定\n",
    "  c = y_train,       # データポイントの色を出力=品種に対応付ける\n",
    "  figsize = (15, 15),# 画像出力サイズの指定(なくてもいい)\n",
    "  marker = 'o',      # ポイントマーカーの指定(なくてもいい)\n",
    "  hist_kwds = {'bins': 20}, # ヒストグラムを作る関数に渡す引数の指定(とりあえずなくてもいい)\n",
    "  s = 60,            # データポイントのサイズ？(なくてもいい)\n",
    "  alpha = .8,        # 透過度調整(なくてもいい)\n",
    "  cmap = mglearn.cm3 # 配色設定(カラーマップ、なくてもいい)\n",
    ")\n",
    "## 表示\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![](png1/unnamed-chunk-13-1.png)<!-- -->\n",
    "\n",
    "### 最初のモデル: k-最近傍法\n",
    "\n",
    "- **距離的に近いやつは仲間でいいんじゃね？**に基づくアルゴリズム。\n",
    "- **scikit-learn**のアルゴリズムを使うためには、アルゴリズムに対応するオブジェクトのインスタンスを生成する必要がある。\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) # インスタンス生成時にパラメータを指定できるものもある\n",
    "```\n",
    "\n",
    "モデルに訓練セットを適合させるためには、`fit`メソッドを呼び出して訓練セットを渡すだけで良い。\n",
    "\n",
    "\n",
    "```python\n",
    "knn.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 予測を行う\n",
    "\n",
    "予測は`predict`メソッドで行う。適当にデータを生成して予測してみよう。予測結果は符号化された値になるが、`iris_dataset.target_names`を使うと実際のラベル名が分かる。\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(knn.predict(X_new))\n",
    " ## [0]\n",
    "print(iris_dataset.target_names[knn.predict(X_new)])\n",
    " ## ['setosa']\n",
    "```\n",
    "\n",
    "- 適当な数字から予測したから**正解なのかどうか分からない！**\n",
    "- そのためのテストデータ。\n",
    "\n",
    "### モデルの評価\n",
    "\n",
    "- **精度 (accuracy)**: テストデータのラベルを正しく判別できた割合。\n",
    "- テストデータを使って予測を行い、正解と同じラベルがどれだけあるか、をカウントする。\n",
    "\n",
    "\n",
    "```python\n",
    "# 予測\n",
    "y_pred = knn.predict(X_test)\n",
    "# 比較\n",
    "print(y_pred == y_test)\n",
    " ## [ True  True  True  True  True  True  True  True  True  True  True  True\n",
    " ##   True  True  True  True  True  True  True  True  True  True  True  True\n",
    " ##   True  True  True  True  True  True  True  True  True  True  True  True\n",
    " ##   True False]\n",
    "```\n",
    "\n",
    "pythonは数値計算の際Trueは1、Falseは0として扱うので、`y_pred == y_test`の平均値がそのまま精度になる。\n",
    "\n",
    "\n",
    "```python\n",
    "print(np.mean(y_pred == y_test))\n",
    " ## 0.9736842105263158\n",
    "```\n",
    "\n",
    "予測から精度計算まで一発でやってくれるメソッドとして`score`もある。\n",
    "\n",
    "\n",
    "```python\n",
    "print(knn.score(X_test, y_test))\n",
    " ## 0.9736842105263158\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
